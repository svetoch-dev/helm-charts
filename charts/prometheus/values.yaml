global:
  rbac:
    create: true
    pspEnabled: true
    pspAnnotations: {}

  imagePullSecrets: []

ingresses:
  alertmanager-main:
    enabled: false
    name: '{{ include "prometheus.fullname" . }}-alertmanager-main'
    service:
      name: '{{ include "prometheus.fullname" . }}-alertmanager-main'
      port: 80
    className: "{{ .Values.global.ingress.class }}"
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-pomerium
      ingress.pomerium.io/policy: |
        {{- tpl .Values.global.pomerium.policy.default $ | indent 2}}
    hosts:
      - host: "al.{{ .Values.global.company.domain.env }}"
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls:
      - secretName: al-tls
        hosts:
          - "al.{{ .Values.global.company.domain.env }}"
  prometheus-main:
    enabled: false
    name: '{{ include "prometheus.fullname" . }}-prometheus-main'
    service:
      name: '{{ include "prometheus.fullname" . }}-prometheus-main'
      port: 80
    className: "{{ .Values.global.ingress.class }}"
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-pomerium
      ingress.pomerium.io/policy: |
        {{- tpl .Values.global.pomerium.policy.default $ | indent 2}}
    hosts:
      - host: "pm.{{ .Values.global.company.domain.env }}"
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls:
      - secretName: pm-tls
        hosts:
          - "pm.{{ .Values.global.company.domain.env }}"
  prometheus-main-thanos:
    enabled: false
    name: '{{ include "prometheus.fullname" . }}-prometheus-main-thanos'
    service:
      name: '{{ include "prometheus.fullname" . }}-prometheus-main-thanos'
      port: 8080
    className: konghq
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-konghq
      konghq.com/protocols: grpc,grpcs
      konghq.com/plugins: thanos-main-ip-restriction
    hosts:
      - host: "thanos.{{ .Values.global.company.domain.env }}"
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls:
      - secretName: thanos-tls
        hosts:
          - "thanos.{{ .Values.global.company.domain.env }}"


alertmanager-configs:
  enabled: false
  configs:
    main:
      enabled: false
      selector:
        alertmanager: main
      route:
        receiver: default
        continue: false
        routes:
          #Default route
          - receiver: "default"
            matchers:
              - name: env
                value: "{{ .Values.global.env.short_name }}"
                matchType: "="
              - name: team
                value: "sre"
                matchType: "="
              - name: severity
                value: "(low|medium|high|critical)"
                matchType: "=~"
      inhibitRules:
        - sourceMatch:
            - name: severity
              value: "critical"
          targetMatch:
            - name: severity
              value: "medium"
          equal: ['alertname', 'persistentvolumeclaim']
        - sourceMatch:
            - name: alertname
              value: "OOM_killed_pods"
          targetMatch:
            - name: alertname
              value: "OOM_killed_pods_(on_node)"
          equal: ['node']
        - sourceMatch:
            - name: alertname
              value: "OOM_killed_pods"
          targetMatch:
            - name: alertname
              value: "High_memory_usage"
          equal: ['container', 'pod']
        - sourceMatch:
            - name: alertname
              value: "Pod_replicas_not_ready"
          targetMatch:
            - name: alertname
              value: "Pods_waiting_state"
          equal: ['label_app_kubernetes_io_name']
        - sourceMatch:
            - name: alertname
              value: "Pods_waiting_state"
          targetMatch:
            - name: alertname
              value: "Container_too_many_restarts"
          equal: ['container']
        - sourceMatch:
            - name: alertname
              value: "Pod_replicas_not_ready"
          targetMatch:
            - name: alertname
              value: "Container_too_many_restarts"
          equal: ['label_app_kubernetes_io_name']
        - sourceMatch:
            - name: alertname
              value: "Postgres_is_down"
          targetMatch:
            - name: alertname
              value: "Postgres_replica_is_down"
          equal: ['label_app_kubernetes_io_instance']
        - sourceMatch:
            - name: alertname
              value: "Redis_too_many_masters"
          targetMatch:
            - name: alertname
              value: "Redis_disconnected_slaves"
          equal: ['job']
        - sourceMatch:
            - name: alertname
              value: "Chainlink_multi_node_states"
          targetMatch:
            - name: alertname
              value: "Chainlink_node_states"
          equal: ['chainId']

prometheus-node-exporter:
  enabled: false
  extraArgs:
    - '--log.format=json'
  kubeRBACProxy:
    enabled: false
  resources:
    limits:
      cpu: 250m
      memory: 50Mi
    requests:
      cpu: 10m
      memory: 20Mi
  service:
    enabled: true
    annotations: {}
  prometheus:
    monitor:
      enabled: true
      additionalLabels:
        prometheus: main
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          action: Replace
          regex: (.*)
          targetLabel: node

  releaseLabel: false
  serviceAccount:
    create: false
  verticalPodAutoscaler:
    enabled: false
  nodeSelector:
    kubernetes.io/os: linux

kube-state-metrics:
  enabled: false
  prometheusScrape: false
  resources:
    limits:
      cpu: 100m
      memory: 100Mi
    requests:
      cpu: 10m
      memory: 20Mi
  image:
    tag: v2.16.0
  serviceAccount:
    automountServiceAccountToken: false
  extraArgs:
    - --metric-labels-allowlist=pods=[*],deployments=[*],nodes=[*]
  collectors:
    # - certificatesigningrequests
    # - configmaps
    - cronjobs
    - daemonsets
    - deployments
    # - endpoints
    # - horizontalpodautoscalers
    # - ingresses
    # - jobs
    - leases
    # - limitranges
    # - mutatingwebhookconfigurations
    # - namespaces
    # - networkpolicies
    - nodes
    - persistentvolumeclaims
    - persistentvolumes
    # - poddisruptionbudgets
    - pods
    # - replicasets
    # - replicationcontrollers
    # - resourcequotas
    # - secrets
    # - services
    - statefulsets
    # - storageclasses
    # - validatingwebhookconfigurations
    # - volumeattachments
    # - ingressclasses
    # - clusterrolebindings
    # - clusterroles
    # - roles

  prometheus:
    monitor:
      enabled: true
      namespace: prometheus
      namespaceSelector:
        - prometheus
      additionalLabels:
        prometheus: main
      http:
        metricRelabelings:
          # ADD COMMON pod and LABEL TO ALL METRICS
          # values based on orgiginal exported_pod label (container also)
          - sourceLabels: ["exported_pod"]
            targetLabel: "pod"
          - sourceLabels: ["exported_container"]
            targetLabel: "container"
          - sourceLabels: ["exported_namespace"]
            targetLabel: "namespace"
          - regex: "exported_namespace|exported_pod|exported_container"
            action: "labeldrop"
          # Abstract away from cloud related node
          # labels
          - sourceLabels: ["label_cloud_google_com_gke_nodepool"]
            targetLabel: "nodepool"

operator:
  enabled: false

  commonLabels:
    prometheus: main

  defaultRules:
    create: false
  grafana:
    enabled: false
  kubeApiServer:
    enabled: false
  kubelet:
    enabled: true
    namespace: prometheus
    serviceMonitor:
      cAdvisorMetricRelabelings:
        # Drop cgroup metrics with no pod.
        - sourceLabels: [id, pod]
          action: drop
          regex: ".+;"
  kubeControllerManager:
    enabled: false
  coreDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeProxy:
    enabled: false
  kubeStateMetrics:
    enabled: false
  nodeExporter:
    enabled: false
  prometheus:
    enabled: false
  alertmanager:
    enabled: false

  prometheusOperator:
    enabled: false
    logFormat: json
    admissionWebhooks:
      enabled: false
    tls:
      enabled: false
    kubeletService:
      enabled: true
      namespace: prometheus
    resources:
      limits:
        memory: 150Mi
        cpu: 200m
      requests:
        cpu: 10m
        memory: 30Mi
    prometheusConfigReloader:
      resources:
        limits:
          cpu: 200m
          memory: 100Mi
        requests:
          cpu: 10m
          memory: 20Mi

prometheus-blackbox-exporter:
  enabled: false
  extraArgs:
    - '--log.format=json'
  resources:
    limits:
      cpu: 100m
      memory: 100Mi
    requests:
      cpu: 10m
      memory: 15Mi
  config:
    modules:
      http_2xx:
        prober: http
        timeout: 30s
        http:
          valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
          valid_status_codes: [200]
          method: GET
          no_follow_redirects: true
          fail_if_not_ssl: true
          headers:
            X-Healthcheck-Agent: "blackbox-exporter"
      http_3xx:
        prober: http
        timeout: 30s
        http:
          valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
          valid_status_codes: [301, 302]
          method: GET
          no_follow_redirects: true
          fail_if_not_ssl: false
          headers:
            X-Healthcheck-Agent: "blackbox-exporter"

probes:
  enabled: false
  probes: {}

kong-plugins:
  enabled: false
  plugins:
    - name: thanos-main-ip-restriction
      plugin_name: ip-restriction
      annotations:
        kubernetes.io/ingress.class: konghq
      config:
        allow:
          - "{{ .Values.global.network.int_nat_gw }}"

prometheus-main:
  enabled: true
  service:
    enabled: true

  serviceAccount:
    create: false
    name: thanos

  servicemonitor:
    enabled: true
    prometheusSelector:
      prometheus: main
    endpoints:
      - port: http
        path: /metrics

  thanosService:
    enabled: true
    annotations:
      konghq.com/protocol: grpc

  thanosServiceMonitor:
    enabled: true
    prometheusSelector:
      prometheus: main
    endpoints:
      - port: http
        path: /metrics

  prometheus:
    spec:
      logFormat: json
      enableAdminAPI: true
      thanos:
        image: quay.io/thanos/thanos:v0.39.2@sha256:1d022ef4b8eff056a0e3b7822f953d931c5704d068413f2d7ce5266aa96c9e80
        version: v0.39.2
        objectStorageConfig:
          key: objstore.yml
          name: thanos
        resources:
          limits:
            cpu: 350m
            memory: 150Mi
          requests:
            cpu: 10m
            memory: 35Mi

      externalLabels:
        env: "{{ .Values.global.env.name }}"

      ruleSelector:
        matchLabels:
          prometheus: main

      serviceMonitorSelector:
        matchLabels:
          prometheus: main

      podMonitorSelector:
        matchLabels:
          prometheus: main

      probeSelector:
        matchLabels:
          prometheus: main

      retention: 8h

      alerting:
        alertmanagers:
          - name: "{{ .Release.Name }}-alertmanager-main"
            namespace: prometheus
            port: http

      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 50m
          memory: 300Mi

      storage:
        volumeClaimTemplate:
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: "2Gi"

alertmanager-main:
  enabled: false

  service:
    enabled: true

  config:
    global:
      resolve_timeout: 30m
    route:
      receiver: 'null'
      group_by: [env, alertname]
      group_wait: 30s
      group_interval: 10m
      repeat_interval: 3h
    receivers:
      - name: 'null'
    templates:
      - "/etc/alertmanager/config/*.tmpl"

  servicemonitor:
    enabled: true
    prometheusSelector:
      prometheus: main
    endpoints:
      - port: http
        path: /metrics
  alertmanager:
    spec:
      logFormat: json
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 10m
          memory: 25Mi
      alertmanagerConfigSelector:
        matchLabels:
          alertmanager: main

prometheus-postgres-exporter:
  enabled: false

prometheus-stackdriver-exporter:
  enabled: false
  extraArgs:
    log.format: json
  resources:
    limits:
      cpu: 300m
      memory: 100Mi
    requests:
      cpu: 10m
      memory: 20Mi
  serviceAccount:
    create: false
    name: 'stackdriver-exporter'
  serviceMonitor:
    enabled: true
    namespace: prometheus
    additionalLabels:
      prometheus: main

prometheus-rules:
  enabled: false
  PrometheusRules: {}
  PrometheusAlerts:
    prometheus-main:
      env: "{{ .Values.global.env.short_name }}"
      team: sre
      selector:
        prometheus: main

      AbsentMetricCritical:
        enabled: false
        rules:
          kong_http_requests_total:
            enabled: false
            source: "kong exporter"
          pg_stat_activity_count:
            enabled: false
            source: "prometheus-portgres-exporter"
          pg_stat_database_xact_commit:
            query: "{datname!~'^(postgres|template[0-9]*|default|cloudsqladmin|unknown)$'}"
            enabled: false
            source: "prometheus-portgres-exporter"
          pg_up:
            enabled: false
            source: "prometheus-portgres-exporter"
          probe_success:
            enabled: false
            source: "prometheus-blackbox-exporter"

      CriticalMetric:
        enabled: false
        interval: "31s"
        rules:
          # Blackbox
          api:
            enabled: false
            expr: probe_success == 0
            for: "2m"
            description: "URL {{ $labels.instance }} is down/unreachable"
            summary: "{{ $labels.instance }} is down"

          # Chainlink node
          Chainlink_multi_node_states:
            enabled: false
            expr: sum by (chainId) (chainlink_multi_node_states{state!="Alive"} > 0) > 1
            for: "0m"
            keep_firing_for: "15m"
            description: "There are {{ $value }} EVM Nodes with chainId {{ $labels.chainId }} is down"

          # K8s
          Daemonset_not_ready:
            enabled: false
            for: "15m"
            description: "daemonset '{{ $labels.daemonset }}' in {{ $labels.namespace }} not scheduled in 15m"
            expr: kube_daemonset_status_number_misscheduled > 1
          Pod_replicas_not_ready:
            enabled: false
            for: "15m"
            description: "Pods replicas less than specified for '{{ $labels.deployment }}' in {{ $labels.namespace }} for > 15m"
            expr: (kube_deployment_spec_replicas != kube_deployment_status_replicas_available) * on (namespace, deployment) group_left (label_app_kubernetes_io_name) kube_deployment_labels
          Pods_waiting_state:
            enabled: false
            for: "20m"
            description: "Container {{ $labels.container }} in {{ $labels.pod }} in namespace {{ $labels.namespace }} waiting with reason {{ $labels.reason }} for >20 minutes"
            expr: sum by (pod, container, namespace, reason) (kube_pod_container_status_waiting_reason{namespace!="kube-system", reason!="OOMKilled"}) * on (namespace, pod) group_left (label_app_kubernetes_io_name) kube_pod_labels > 0
          Pods_waiting_state_kube-system:
            enabled: false
            for: "1h"
            description: "Container {{ $labels.container }} in {{ $labels.pod }} in namespace {{ $labels.namespace }} waiting with reason {{ $labels.reason }} for >1h"
            expr: sum by (pod, container, namespace, reason) (kube_pod_container_status_waiting_reason{namespace="kube-system", reason!="OOMKilled"}) > 0
          Statefulset_not_ready:
            enabled: false
            for: "15m"
            description: "StatefulSet {{ $labels.statefulset }} in namespace {{ $labels.namespace }} has not matched the expected number of replicas for >15 minutes"
            expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas

          # Kong (nginx)
          ErrorRateIncreasedFor_5XX:
            enabled: false
            expr: sum by (exported_service) (rate(kong_http_requests_total{code=~"5.."}[10m])) / sum by (exported_service) (rate(kong_http_requests_total[10m])) > 0.1
            for: "10m"
            description: "KongHQ error rate is high for {{ $labels.exported_service }} service"

          # Postgres
          Available_postgresql_connections_are_running_out:
            enabled: false
            for: "15m"
            description: "There are {{ $value }} available postgres (pod {{ $labels.pod }}) connections left, located in namespace {{ $labels.namespace }}"
            expr: (sum by (pod, namespace) (pg_settings_max_connections) - sum by (pod, namespace) (pg_stat_activity_count{container=~"^.*[^0-9]$"}) < 30) or (sum by (pod, namespace) (pg_stat_activity_count{container=~"^.*[^0-9]$"}) / sum by (pod, namespace) (pg_settings_max_connections) > 0.90)
          DB_transactions_per_seconds_below_norm:
          # There are some transactions from postgres exporter
          # so we the Transactions per second cant be 0 and we need to
          # check against a small average number like 0.5
            enabled: false
            expr: sum by (datname, label_app_kubernetes_io_instance) (rate(pg_stat_database_xact_commit{datname!~"^(postgres|template[0-9]*|default|cloudsqladmin|unknown)$"}[5m])) < 0.5
            for: "2m"
            description: "Transactions per second for database {{ $labels.datname }} in {{ $labels.label_app_kubernetes_io_instance }} service is below norm"
          Postgres_is_down:
            enabled: false
            for: "5m"
            description: "Postgres {{ $labels.pod }} in {{ $labels.namespace }} is down"
            expr: pg_up == 0
          Postgres_replica_is_down:
            enabled: false
            for: "5m"
            description: "Postgres replica {{ $labels.label_app_kubernetes_io_instance }} in {{ $labels.namespace }} is down"
            expr: sum by (label_app_kubernetes_io_instance, namespace) (pg_replication_is_replica) < 1

          # Redis
          Redis_missing_master:
            enabled: false
            for: "5m"
            description: "Redis cluster '{{ $labels.job }}' has no node marked as master"
            expr: count by (job) (redis_instance_info{role="master"}) == 0
          Redis_too_many_masters:
            enabled: false
            for: "5m"
            description: "Redis cluster '{{ $labels.job }}' has too many({{ $value }}) masters"
            expr: count by (job) (redis_instance_info{role="master"}) > 1

          # Resources
          OOM_killed_pods:
            enabled: false
            for: "0m"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} located in namespace {{ $labels.namespace }} (node {{ $labels.node }}) was killed by OOM killer"
            expr: ((increase(kube_pod_container_status_restarts_total[10m]) > 0) and ignoring (reason) kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}) * on (pod) group_left (node) kube_pod_info > 0
          PVC_low_capacity:
            enabled: false
            for: "15m"
            description: "Volume {{ $labels.persistentvolumeclaim }} located in namespace {{ $labels.namespace }} (on {{ $labels.node }}) has capacity {{ humanizePercentage $value }}"
            expr: (kubelet_volume_stats_available_bytes / on(instance,namespace,node,persistentvolumeclaim,service) kubelet_volume_stats_capacity_bytes) < 0.05

          # RMQ
          RabbitMQ_High_memory(watermark)_usage:
            enabled: false
            for: "20m"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} located in namespace {{ $labels.namespace }} is using {{ humanizePercentage $value }} of 'Memory high watermark' limit for > 20m"
            expr: (sum by (container,pod,namespace) (container_memory_working_set_bytes{container="rabbitmq"}) / on(container,pod,namespace) max by (container,pod,namespace) (rabbitmq_detailed_resident_memory_limit_bytes)) > 0.9
          RabbitMQ_missing_master:
            enabled: false
            for: "5m"
            description: "RabbitMQ in {{ $labels.vhost }} has no master"
            expr: max by (vhost) (count by (vhost, queue) (rabbitmq_detailed_queue_messages)) == 0
          RabbitMQ_too_many_masters:
            enabled: false
            for: "5m"
            description: "RabbitMQ in {{ $labels.vhost }} has to many({{ $value }}) masters"
            expr: max by (vhost) (count by (vhost, queue) (rabbitmq_detailed_queue_messages)) > 1

      WarningMetric:
        enabled: false
        interval: "31s"
        rules:
          # Chainlink node
          Chainlink_node_states:
            enabled: false
            for: "5m"
            expr: chainlink_multi_node_states{state!="Alive"} > 0
            keep_firing_for: "15m"
            description: "EVM Node ({{ $labels.network }} chainId {{ $labels.chainId }}, container {{ $labels.container }}) is down"

          # Deployment
          Container_too_many_restarts:
            enabled: false
            for: "10m"
            keep_firing_for: "12m"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} located in namespace {{ $labels.namespace }} too many restarts (>10 for 10m)"
            expr: increase(kube_pod_container_status_restarts_total{namespace!="kube-system"}[5m]) * on (namespace, pod) group_left (label_app_kubernetes_io_name) kube_pod_labels > 10

          # GCP
          To_many_gcp_logs:
            enabled: false
            for: "5m"
            description: "Container {{ $labels.container_name }} in pod {{ $labels.pod_name }} located in namespace {{ $labels.namespace_name }} sends to many logs ({{ $labels.log }})"
            # since stackdriver has a problem with obtaining Delta metrics and the metrics are not accurate, we experimentally found that dividing the result by 5 gives the most accurate results. Therefore, we divide the result by 5
            expr: rate(stackdriver_k_8_s_container_logging_googleapis_com_log_entry_count[10m])/5 > 1000

          # Postgres
          Postgres_logical_backup_error:
            enabled: false
            for: "2h"
            description: "Cronjob {{ $labels.cronjob }} failed"
            expr: ((kube_cronjob_next_schedule_time - on(job, cronjob, namespace) kube_cronjob_status_last_successful_time)/3600) > 25

          # Redis
          Redis_disconnected_slaves:
            enabled: false
            for: "15m"
            description: "Redis '{{ $labels.job }}' not replicating for all slaves. Consider reviewing the redis replication status."
            expr: count by (job) (redis_instance_info{role="slave"}) < 2

          # Resources
          High_cpu_throttling:
            enabled: false
            for: "1h"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} located in namespace {{ $labels.namespace }} throttled for >1h with throttle of {{ humanizeDuration $value }}"
            expr: rate(container_cpu_cfs_throttled_seconds_total{container!~"postgres"}[5m]) > 0.2
          High_memory_usage:
            enabled: false
            for: "30m"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} located in namespace {{ $labels.namespace }} is using {{ humanizePercentage $value }} of Memory limit for > 30m"
            expr: sum by (container, pod, namespace) (container_memory_working_set_bytes{container!="", namespace!="kube-system"}) / on (container, pod, namespace) max by (container, pod, namespace) (kube_pod_container_resource_limits{unit="byte"}) > 0.9
          High_postgres_container_cpu_throttling:
            enabled: false
            for: "1h"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} located in namespace {{ $labels.namespace }} throttled for >1h with throttle of {{ humanizeDuration $value }}"
            expr: rate(container_cpu_cfs_throttled_seconds_total{container="postgres"}[5m]) > 0.1
          OOM_killed_pods_(on_node):
            enabled: false
            for: "1m"
            description: "Some container located on node {{ $labels.node }} was killed by OOM killer"
            expr: increase(node_vmstat_oom_kill[10m]) > 0
          PVC_low_capacity:
            enabled: false
            for: "1h"
            description: "Volume {{ $labels.persistentvolumeclaim }} located in namespace {{ $labels.namespace }} (on {{ $labels.node }}) has capacity {{ humanizePercentage $value }}"
            expr: (kubelet_volume_stats_available_bytes / on(instance,namespace,node,persistentvolumeclaim,service) kubelet_volume_stats_capacity_bytes) < 0.1
